{"cells":[{"cell_type":"markdown","metadata":{"id":"Z6DEzHC6I_O6"},"source":["# About Feature Scaling and Normalization\n","> The effect of standardization for machine learning algorithms. \n","\n","- toc: true \n","- badges: true\n","- comments: true\n","- categories: [Machine Learning]"]},{"cell_type":"markdown","metadata":{"id":"XGH4gk-UI_O-"},"source":["# About Standardization\n","\n","The result of **standardization** (or **Z-Score normalization**) is that the features will be re scaled so that they'll have the properties of a standard normal distribution with: \n","$$\\mu = 0$$\n","And\n","$$\\sigma = 1$$\n","\n","Where $\\mu$ is the mean(average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called **Z** scores) of the sampels are calculated as follows: \n","$$z = \\frac{x - \\mu}{\\sigma}$$\n","\n","Standardizing the features so that they are centered around $0$ with a standard deviation of $1$ is not only important if we are computing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm) often used in: \n","\n","- `Logistic Regression`\n","- `Support Vector Machine`\n","- `Perceptrons`\n","- `Neural Network`\n","\n","with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates:\n","$$\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\sum_i (t^{i} - o^{i})x_j^{i}$$\n","\n","So that $w_j := w_j + \\Delta w_j$, where $\\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include: \n","\n","- `K_Nearest Neighbor`\n","- `Clustering`\n","\n","Alorithms that use, for example: `Euclidean Distance Measures` - in fact, `tree-based` classifier are probably the only classifiers where feature scaling doesn't make a difference. \n","\n","In fact, the only family of algorithms that I could think of being scale-invariant are `tree-based` method. \n","\n","Let's take a general `CART Dicision Tree` algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as `is feature x_i >= some_value`? Intuitively, wen can see that it really doesn't matter on which scale this feature is. \n","\n","Some examples of algorithms where feature scaling matters are:\n","\n","- `k-nearest neighbors` with `Euclidean Distance` measure if want all features to contribute equally. \n","- `k-means` similar to `k-nearest neighbors`\n","- `LinearRegression`, `LogisticRegression`, `Support Vector Machine`, `Perceptrons`, `Neural Networks` as long as it using Gradient Descent Optimization. \n","- `Linear Discriminant Analysis(LDA)`, `Princial Component Analysis(PCA)`, as long as it want to find directions of maximizing the variance (under the constraints that those directions/`eigenvectors`/`Principal Components` are orthogonal); you want to have features on the same scale since you'd emphasize variables on \"larger measurement scales\" more. \n","\n","In addition, we'd also want to think about whether we want to **Standardize** or **Normalize**(here: scaling to `[0, 1]` range) our data. Some algorithms assume that our data is centered at `0`. For example, if we initialize the weights of a small multi-layer perceptron with `tanh` activation units to `0` or small random values centered around zero, we want to update the model weights **equally**. As a rule of thumb: _When in doubt, just standardize the data, it shouldn't hurt_\n","***\n","\n","# About Min-Max scaling\n","\n","An alternative approach to **Z-Score** normalization (or called standardization) is the so-called **Min-Max Scaling** (often also simply called **Normalization** - a common cause for ambiguities)\n","\n","In this approach, the data is scaled to a fixed range - usually `[0, 1]`. \n","The cost of having this bounded range - in contrast to standrdization - is that we will end up with smaaller standard deviations, which can suppress the effect of outliers. \n","\n","**Note**:\n","\n","If the dataset have lot's of outliers, and the algorithms are sensitive to outliers, please use `Min-Max Scaler`\n","\n","A `Min-Max Scaling` is typically done via the foloowing equation: \n","\n","$$X_{norm} = \\frac{X_{i} - X_{min}}{X_{max} - X_{min}}$$\n","\n","$X_i$ is the $i^{th}$ sample of dataset. \n","\n","\n","# Z-Score Standardization or Min-Max Scaling\n","\n","\"Standardization or Min-Max scaling\"? - There is no obvious answer to this question: it really depends on the application. \n","\n","For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the `Principal Component Analysis`, where we usually prefer `Standardization` over `Min-Max Scaling`, since we are interested in the components that maximize the variance(depending on the question and if the PCA computes the components via the correaltion matrix instead of the covariance matrix)\n","\n","However this doesn't mean that `Min-Max Scaling` is not useful at all, A popular application is `image processing`, where pixel intensities have to be normalized to fit withint a certain range (i.e., `[0, 255]` for the RGB colour range). Also, typical _Neural Network_ Algorithm require data that on a `0 - 1` scale. \n","\n","# Standardizing and Normalizing - How it can be done using `scikit-learn`\n","\n","Of course, we could make use of NumPy's vectorization capabilities to calculate the `Z-Score` for standardization and normalize the data using the equations that were mentioned in the previous sections. However, there is even more convenient aapproach using the preprocessing module from one of Python's open-source maachine learning library [scikit-learn](http://scikit-learn.org/)\n","\n","For the following examples anad discussing, we will have a look at free `Wine` Dataset that is deposited on the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)\n","\n","> Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.\n","\n","> Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n","\n","The `Wine` dataset consists of `3` different classes where each row correspond to aa particular wine sample. \n","\n","The class label `(1, 2, 3)` are listed in the `1st` column, and the columns `2 - 14` correspond to `13` different attributes(features): \n","\n","- Alcohol\n","- Malic Acid\n","- \n","...\n","\n","## Loading the wine dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rib5XpR2I_PB"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n","\n","df = pd.read_csv(url, header=None, usecols=[0, 1, 2])\n","df.columns = ['Label', 'Alcohol', 'MalicAcid']\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44VtoepLI_PD"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"4Xni8cqEI_PD"},"source":["As we can see in the table above, the features **Aalcohol**(per cent/Volume) and **Malic Acid**(g/l) are meaasured on different scales, so that **Feature Scaling** is necessary important perior to any comparison or combination of these data. \n","\n","## Standardization and Min-Max Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIgQ8pz4I_PE"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","standard_scale = StandardScaler()\n","df_stdScale = standard_scale.fit_transform(df[['Alcohol', 'MalicAcid']])\n","df_stdScale[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riVdhW3LI_PE"},"outputs":[],"source":["minmax_scale = MinMaxScaler()\n","df_minmax = minmax_scale.fit_transform(df[['Alcohol', 'MalicAcid']])\n","df_minmax[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEoU-p9eI_PF"},"outputs":[],"source":["print(f\"The mean of Standard Scaling: {np.mean(df_stdScale):.2f}\")\n","print(f\"The Standard Deviation of Standard Scaling: {np.std(df_stdScale):.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lJ98oqAI_PG"},"outputs":[],"source":["print(f\"The min of Min-Max Scaling: {df_minmax.min():.2f}\")\n","print(f\"The max of Min-Max Scaling: {df_minmax.max():.2f}\")\n","print(f\"The mean of Min-Max Scaling: {np.mean(df_minmax):.2f}\")\n","print(f\"The Standard Deviation of Min-Max Scaling: {np.std(df_minmax):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"d0ZTsp1yI_PG"},"source":["## Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NXveoZtI_PH"},"outputs":[],"source":["%matplotlib inline\n","\n","from matplotlib import pyplot as plt\n","\n","def plot():\n","    plt.figure(figsize=(8,6))\n","    plt.scatter(df['Alcohol'], df['MalicAcid'],\n","                color='green', label='Input Scale', alpha=0.5)\n","    plt.scatter(df_stdScale[:, 0], df_stdScale[:, 1],\n","                color='red', label='Standardized [mean=0, std=1]', alpha=0.3)\n","    plt.scatter(df_minmax[:, 0], df_minmax[:, 1], label='min-max scaled [min=0, max=1]', alpha=0.3)\n","    \n","    plt.title('Alcohol and Malic Acid content of the wine dataset')\n","    plt.xlabel('Alcohol')\n","    plt.ylabel('Malic Acid')\n","    plt.legend(loc='upper left')\n","    plt.grid()\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFtBQAuKI_PI"},"outputs":[],"source":["plot()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GuJikE3XI_PI"},"source":["The plot above includes the wine datapoints on all `3` different scale: \n","- **Input Scale** Original Input Scale\n","- **Standardized** Using `sklearn` `StandardScaler`\n","- **min-max scaled** Using `sklearn` `MinMaxScaler`\n","\n","Next Plot we will zoom in each of `3` scale to see the label distribution, they should have the same location/distribution, but just the difference of scale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1ck1E5wI_PJ"},"outputs":[],"source":["fig, ax = plt.subplots(3, figsize=(6,14))\n","\n","for a,d,l in zip(range(len(ax)),\n","               (df[['Alcohol', 'MalicAcid']].values, df_stdScale, df_minmax),\n","               ('Input scale',\n","                'Standardized [mean=0, std=1]',\n","                'min-max scaled [min=0, max=1]')\n","                ):\n","    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n","        ax[a].scatter(d[df['Label'].values == i, 0],\n","                  d[df['Label'].values == i, 1],\n","                  alpha=0.5,\n","                  color=c,\n","                  label='Class %s' %i\n","                  )\n","    ax[a].set_title(l)\n","    ax[a].set_xlabel('Alcohol')\n","    ax[a].set_ylabel('Malic Acid')\n","    ax[a].legend(loc='upper left')\n","    ax[a].grid()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Rrj9RmMbI_PJ"},"source":["# Bottom-up Approaches\n","\n","We can also code the equations for standardization and `[0, 1]` Min-Max scaling _Manually_. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally. \n","\n","```python\n","std_scale = preprocessing.StandardScaler().fit(X_train)\n","X_train = std_scale.transform(X_train)\n","X_test = std_scale.transform(X_test)\n","```\n","\n","Here we try to perform the computation using native Python code, and more convenient `NumPy` solution, which is especially useful if we attempt to transform the whole matrix. \n","\n","Recall the equations: \n","\n","- **Standardization**:\n","$$z = \\frac{x - \\mu}{\\sigma}$$\n","\n","- **Mean:**\n","$$\\mu = \\frac{1}{N}\\sum_{i=1}^{N}(x_i)$$\n","\n","- **Standard Deviation:**\n","$$\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2}$$\n","\n","- **Min-Max Scaling:**\n","$$X_{norm} = \\frac{X_i - X_{min}}{X_{max} - X_{min}}$$\n","\n","## Native Python\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ba0aJulhI_PJ"},"outputs":[],"source":["# Standardization\n","\n","x = [1, 4, 5, 6, 6, 2, 3]\n","mean = sum(x)/len(x)\n","\n","std_dev = (1/len(x) * sum([(x_i - mean)**2 for x_i in x]))**0.5\n","\n","z_scores = [(x_i - mean)/std_dev for x_i in x]\n","\n","print(\"Native Python\")\n","print(f\"Array X: {x}\\n\")\n","print(f\"Array X mean: {mean}\\n\")\n","print(f\"Array X Standard Deviation: {std_dev}\\n\")\n","print(f\"Array X Z Scores: {z_scores}\\n\")\n","\n","print(\"NumPy for cross validation\")\n","print(f\"Numpy Array: {x}\\n\")\n","print(f\"Numpy Array mean: {np.mean(x)}\\n\")\n","print(f\"Numpy Array Standard Deviation: {np.std(x)}\\n\")\n","print(f\"Numpy Z Scores: {[(x_i - np.mean(x))/np.std(x) for x_i in x]}\\n\")\n","\n","print(f\"Min-Max: {[(x_i - min(x))/ (max(x) - min(x)) for x_i in x]}\")"]},{"cell_type":"markdown","metadata":{"id":"PhEwQEGII_PK"},"source":["For some dataset, `Min-Max Scaling` will create some, or lot's of Sparse value (lot's of `0`) and some values will be truncated (capped at `1`), so please take note on this. \n","\n","## NumPy\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tj7p3NWQI_PK"},"outputs":[],"source":["import numpy as np\n","\n","# Standardization\n","\n","x_np = np.array(x)\n","\n","z_scores_np = (x_np - x_np.mean()) / x_np.std()\n","\n","print(f\"X NumPy array: {x_np}\\n\")\n","print(f\"Z scores in Numpy: \\n{z_scores_np}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNw4RzapI_PK"},"outputs":[],"source":["# min-max scaling\n","np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())\n","print(f\"Min-Max scaling in Numpy: \\n{np_minmax}\")"]},{"cell_type":"markdown","metadata":{"id":"YY5hY0XMI_PL"},"source":["# The effect of standardization on PCA in a pattern classification task\n","\n","\n","Previous chapter we have mentioned the `Principal Component Analysis(PCA)` as an example where standardization is crucial, since it is _analyzing_ the variances of the different features. Now, let's see how the standardization affects `PCA` and a following supervised classification on the whole wine dataset. \n","\n","We will go through the following steps: \n","\n","- `read_csv` the whole dataset\n","- `train_test_split` split the dataset into `training` set and `testing` set. \n","- Standardization the features\n","- `PCA` to reduce dimensionality\n","- Train a classifier\n","- Evaluate the performance **with** and **without** standardization. \n","\n","## Read full dataset\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPqvuc-zI_PL"},"outputs":[],"source":["df = pd.read_csv(url, header=None)\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4Tht444I_PL"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"CVgKBAvUI_PL"},"source":["## Spliting the dataset into `train` set and `test` set\n","\n","We will use `sklearn`'s `train_test_split` function to randomly split the wine dataset into `training` set and `test` set where the `training` set will contain `70%` of the samples and the `test` set will contain `30%` respectively. \n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avJ403tvI_PM"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df.values[:, 1:]\n","y = df.values[:, 0]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n","print(f\"The shape of X_train is: {X_train.shape}\\n\")\n","print(f\"The shape of X_test is {X_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"VMQnzvl2I_PM"},"source":["## Feature Scaling -- Standardization\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQ4o8elgI_PM"},"outputs":[],"source":["from sklearn import preprocessing\n","\n","std_scale = preprocessing.StandardScaler().fit(X_train)\n","X_train_std = std_scale.transform(X_train)\n","X_test_std = std_scale.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"68Y63hwOI_PM"},"source":["## Dimensionality reduction via Principal Component Analysis(PCA)\n","\n","Now, we perform PCA on the `standardized` and the `non-standardized` datasets to transform the dataset onto a **2-dimensional** feature subspace. \n","\n","In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between **Preserving Information** and **Overfitting** for different classfiers. However, we will omit this step since we don't want to train a perfect classifier here, but merely compare the effects of `standardization`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXk0MhqqI_PM"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# on non-standardized data\n","pca = PCA(n_components=2).fit(X_train)\n","X_train = pca.transform(X_train)\n","X_test = pca.transform(X_test)\n","\n","\n","# om standardized data\n","pca_std = PCA(n_components=2).fit(X_train_std)\n","X_train_std = pca_std.transform(X_train_std)\n","X_test_std = pca_std.transform(X_test_std)"]},{"cell_type":"markdown","metadata":{"id":"yQ0J5bKKI_PN"},"source":["Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GS3dWUpI_PN"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\n","\n","\n","for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n","    ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1],\n","        color=c,\n","        label='class %s' %l,\n","        alpha=0.5,\n","        marker=m\n","        )\n","\n","for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n","    ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1],\n","        color=c,\n","        label='class %s' %l,\n","        alpha=0.5,\n","        marker=m\n","        )\n","\n","ax1.set_title('Transformed NON-standardized training dataset after PCA')    \n","ax2.set_title('Transformed standardized training dataset after PCA')    \n","\n","for ax in (ax1, ax2):\n","\n","    ax.set_xlabel('1st principal component')\n","    ax.set_ylabel('2nd principal component')\n","    ax.legend(loc='upper right')\n","    ax.grid()\n","plt.tight_layout()\n","\n","plt.show()  "]},{"cell_type":"markdown","metadata":{"id":"Z7chuVSrI_PN"},"source":["Before we train a classifier to classifi the class, we can see clearly from the plot that, on the left, the `training` set **without Standardization** , would be difficult to find the decision boundary. But on the right, the `training` set **with Standardization**, the decision boundary is much clearer than left one. \n","\n","## Train a naive Bayes Classifier\n","\n","We will use a naive Bayes classifier for the classification task. If you not familiar with it, the term **naive** comes from the assumption that all features are **independent** with no interference at all, which almost impossible in real-world. \n","it works based on Bayes' Rule: \n","\n","$$P(w_j | x) = \\frac{p(x | w_j) \\times P(w_j)}{p(x)}$$\n","\n","where:\n","\n","- $w$: class label. \n","- $P(w|x)$: posterior probability\n","- $p(x|w)$: prior probability (or likelihood)\n","\n","And the **Decesion Rule:**\n","\n","$$w_1$$ if $$P(w_1|x) > P(w_2|x)$$ else $$w_2$$\n","\n","$$=\\frac{p(x|w_1) \\times P(w_1)}{p(x)} > \\frac{p(x|w_2) \\times P(w_2)}{p(x)}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X7eJbp7I_PN"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","\n","# on non-standardized data\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","\n","# on standardized data\n","gnb_std = GaussianNB()\n","gnb_std.fit(X_train_std, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vd2yGCt-I_PN"},"outputs":[],"source":["from sklearn import metrics\n","\n","pred_train = gnb.predict(X_train)\n","\n","print('\\nPrediction accuracy for the training dataset without Standardization')\n","print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train)))\n","\n","pred_test = gnb.predict(X_test)\n","\n","print('\\nPrediction accuracy for the test dataset without Standardization')\n","print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbjIBKX_I_PO"},"outputs":[],"source":["pred_train_std = gnb_std.predict(X_train_std)\n","\n","print('\\nPrediction accuracy for the training dataset with Standardization')\n","print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n","\n","pred_test_std = gnb_std.predict(X_test_std)\n","\n","print('\\nPrediction accuracy for the test dataset with Standardization')\n","print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))"]},{"cell_type":"markdown","metadata":{"id":"a4Yn8JFnI_PO"},"source":["## Other Classifier\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJs-iYzqI_PO"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","clf = DecisionTreeClassifier()\n","clf.fit(X_train, y_train)\n","\n","clf_std = DecisionTreeClassifier()\n","clf_std.fit(X_train_std, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFqwK5RdI_PO"},"outputs":[],"source":["pred_train_clf = clf.predict(X_train)\n","print(f\"\\nPrediction accuracy for the training dataset without Standardization\")\n","print(f\"{metrics.accuracy_score(y_train, pred_train_clf):.2%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xULuoLu5I_PO"},"outputs":[],"source":["pred_train_clf_test = clf.predict(X_test)\n","print(\"\\nPrediction accuracy for the test dataset without Standardization\")\n","print(f\"{metrics.accuracy_score(y_test, pred_train_clf_test):.2%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s609zSLiI_PP"},"outputs":[],"source":["pred_train_clf_std = clf_std.predict(X_train_std)\n","print('\\nPrediction Accuracy for the training dataset with Standardization')\n","print(f\"{metrics.accuracy_score(y_train, pred_train_clf_std):.2%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AQ_eaFlI_PP"},"outputs":[],"source":["pred_train_clf_std_test = clf_std.predict(X_test_std)\n","print('\\nPrediction Accuracy for the test dataset with Standardization')\n","print(f\"{metrics.accuracy_score(y_test, pred_train_clf_std_test):.2%}\")"]},{"cell_type":"markdown","metadata":{"id":"76Cs7sLAI_PP"},"source":["As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.\n","\n","In **Naive Bayes** Classifier, before **Standardization** classifier after `PCA` perform to around $64.81\\%$ accuracy, after **Standardization**, performance increase to $98.15\\%$\n","\n","Similar to `DecesionTreeClassifier`, test set performance increase from $68.52\\%$ to $96.30\\%$"]},{"cell_type":"markdown","metadata":{"id":"dyYpmKHXI_PP"},"source":["# Appendix: The effect of scaling and mean centering of variable prior to PCA\n","***\n","\n","## Mean centering does not affect the covariance matrix\n","Here, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.\n","\n","Let’s assume we have the 2 variables $x$ and $y$. Then the covariance between the attributes is calculated as\n","\n","$$\\sigma_{xy} = \\frac{1}{N-1}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","Let's write the centered variables as: \n","$$x^{'}= x - \\bar{x}$$\n","And\n","$$y^{'}= y - \\bar{y}$$\n","\n","The centered covariance would then be calculated as follows:\n","$$\\sigma^{'}_{xy} = \\frac{1}{n-1}\\sum_i^n(x^{'}_i - \\bar{x}^{'})(y^{'}_i - \\bar{y}^{'})$$\n","\n","But since after centering, $\\bar{x}^{'}=0$ and $\\bar{y}^{'}=0$ we have $\\sigma^{'}_{xy} = \\frac{1}{n-1}\\sum^n_ix_i^{'}y_i^{'}$ which is our original convariance matrix if we resubstitute back the terms $x^{'}=x - \\bar{x}$ and $y^{'} = y - \\bar{y}$\n","\n","Even centering only one variable, let's say $X$ wouldn't affect the covariance: \n","\n","$$\\sigma_{xy} = \\frac{1}{n-1}\\sum_i^n(x_i^{'} - \\bar{x}^{'})(y_i - \\bar{y})$$\n","$$=\\frac{1}{n-1}\\sum_i^n(x_i^{'} - 0)(y_i - \\bar{y})$$\n","$$=\\frac{1}{n-1}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","## Scaling of variables does affect the covariance matrix\n","If one variable is scaled, e.g., from Pounds into Kilogram ($1$ pound = $0.453591$ kg), it does affect the covariance and therefore influences the result of PCA. \n","\n","Let $c$ be the scaling factor for $X$\n","Given that the **original** covariance is computed as: \n","$$\\sigma_{xy} = \\frac{1}{n - 1}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","the covariance after scaling would be computed as: \n","$$\\sigma_{xy}^{'} = \\frac{1}{n-1}\\sum_i^n(c \\times x_i - c \\times \\bar{x})(y_i - \\bar{y})$$\n","$$=\\frac{c}{n-1}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","So we have:\n","$$\\sigma_{xy} = \\frac{\\sigma_{xy}^{'}}{c} = C \\times \\sigma_{xy}$$\n","\n","Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c\\sigma_{xy}$ So if we'd scaled $X$ from pounds to kilograms, the covariance between $X$ and $Y$ will be $0.453592$ times smaller. \n","\n","## Standardizing affects the covariance\n","Standardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.\n","\n","The equation for standardization of a variable is written as\n","\n","$$z=\\frac{x_i - \\bar{x}}{\\sigma}$$\n","\n","The **Original** covariance matrix: \n","\n","$$\\sigma_{xy} = \\frac{1}{n-1}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","\n","And after standardizing both variables: \n","\n","$$x^{'} = \\frac{x - \\bar{x}}{\\sigma_x}$$ and $$y^{'}=\\frac{y-\\bar{y}}{\\sigma_y}$$\n","\n","$$\\sigma^{'}_{xy} = \\frac{1}{n-1}\\sum_i^n(x_i^{'} - 0)(y_i^{'}-0)$$\n","$$=\\frac{1}{n-1}\\sum_i^n(\\frac{x-\\bar{x}}{\\sigma_x})(\\frac{y-\\bar{y}}{\\sigma_y})$$\n","$$=\\frac{1}{(n-1) \\times \\sigma_x\\sigma_y}\\sum_i^n(x_i - \\bar{x})(y_i - \\bar{y})$$\n","$$=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9S-DiJAPI_PQ"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 64-bit ('py377': conda)","language":"python","name":"python37764bitpy377conda2df17884e61a4d57b75c51e11651657f"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Feature Scaling and Normalization.ipynb","provenance":[{"file_id":"https://github.com/JL1829/turbo-funicular/blob/master/_notebooks/2020-09-19-About%20Feature%20Scaling%20and%20Normalization.ipynb","timestamp":1640579228984}]}},"nbformat":4,"nbformat_minor":0}